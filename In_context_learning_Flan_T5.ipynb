{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOw1D1qjYF8KNuE7BEnjqvE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/padmathanumoorthy/gen-ai/blob/main/In_context_learning_Flan_T5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5Wr0JZtcsEV",
        "outputId": "b4b464fa-a48b-442d-9940-a28869680473"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-24.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 23.1.2\n",
            "    Uninstalling pip-23.1.2:\n",
            "      Successfully uninstalled pip-23.1.2\n",
            "Successfully installed pip-24.0\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install pyTorch - Torch lib"
      ],
      "metadata": {
        "id": "n4WV6DgPRzmo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --disable-pip-version-check torch==1.13.1 torchdata==0.5.1 --quiet\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gt7llFMXc6jz",
        "outputId": "3df67c3f-9777-4e9e-a1c0-d2624b261a7b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m903.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchtext 0.16.0 requires torchdata==0.7.0, but you have torchdata 0.5.1 which is incompatible.\n",
            "torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install huggigface library**"
      ],
      "metadata": {
        "id": "m09jsroXR5qd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers==4.27.2 datasets==2.11.0 --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzoblHnTdiG8",
        "outputId": "69010856-cb33-414a-dcb2-952eb907e96a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.7/468.7 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load the datasets, LLM, tokenizer, configurator.**"
      ],
      "metadata": {
        "id": "8N5QC9MKSFpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import GenerationConfig"
      ],
      "metadata": {
        "id": "U73FQP91d1o3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summarize Dialogue without prompt engineering\n",
        "\n",
        "Generating the summary of a dialogue with the pre-trained LLM Flan T5 from Huggingface. The list of available models in the HuggingFace *transformers* package can be found here:\n",
        "\n",
        "Upload simple dialogues from the Dialogsum huggingface dataset.\n",
        "This contains 10000+dialogue and the corresponding manually labeled summaries and topics."
      ],
      "metadata": {
        "id": "7tb6SBmWlLNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
        "#public dataset from huggingface\n",
        "dataset = load_dataset(huggingface_dataset_name)"
      ],
      "metadata": {
        "id": "6tKoJh1oeLpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_indices = [80,300,400]\n",
        "#dash_line = '-'.join('' for x in range(100))\n",
        "dash_line = '-'*100\n",
        "for i, index in enumerate(example_indices):\n",
        "  print(dash_line)\n",
        "  print('Example ',i+1)\n",
        "  print(dash_line)\n",
        "  print('Input Dialogue:')\n",
        "  print(dataset['test'][index]['dialogue'])\n",
        "  print(dash_line)\n",
        "  print('Baseline human summary:')\n",
        "  print(dataset['test'][index]['summary'])\n",
        "  print(dash_line)\n",
        "  print()"
      ],
      "metadata": {
        "id": "qNT_TZM6e0nA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load the Flan t5 model creating an instance of the AutoModelForSeq2SeqLM class with the from_pretrained method\n",
        "model_name = 'google/flan-t5-base'\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "MnwGm741f_LM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To perform encoding and decoding, we need to work with text in a tokenized form.\n",
        "#Tokenization is the process of splitting texts into smaller units that can be\n",
        "#processed by the LLM models\n",
        "\n",
        "#Download the tokenizer for the Flan-t5 model using transformer package AutoTokenizer.from_pretrained() method,\n",
        "#Parameter use_fast switches on fast Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
      ],
      "metadata": {
        "id": "lF_ZSKcBjF5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test the AutoTokenizer instance for its encoding and decoding with a simple sentence\n",
        "\n",
        "dialogue = \"Hello Padma Thanumoorthy, How are you doing?\"\n",
        "dialogue_encoded = tokenizer(dialogue, return_tensors='pt')\n",
        "print('Encoded Dialogue: ')\n",
        "print(dialogue_encoded)\n",
        "dialogue_decoded = tokenizer.decode(dialogue_encoded['input_ids'][0],\n",
        "                                    skip_special_tokens=True)\n",
        "\n",
        "print(dash_line)\n",
        "print('Encoded Dialogue Tensor: Weighted numbers of vector embeddings')\n",
        "print(dialogue_encoded['input_ids'][0])\n",
        "print(dash_line)\n",
        "print('Decoded Dialogue: ')\n",
        "print(dialogue_decoded)\n",
        "print(dash_line)"
      ],
      "metadata": {
        "id": "C8ecfjZGkgMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now its time to explore how well the base FM LLM summarizes a dialogue withiut any prompt enigneering.\n",
        "#Prompt Engineering is an act of a human changing the prommpt (input) to improve the response for a given task\n",
        "\n",
        "for i, index in enumerate(example_indices):\n",
        "  dialogue = dataset['test'][index]['dialogue']\n",
        "  summary = dataset['test'][index]['summary']\n",
        "\n",
        "  inputs = tokenizer(dialogue, return_tensors='pt')\n",
        "  output = tokenizer.decode(\n",
        "      model.generate(inputs['input_ids'], max_new_tokens=50)[0],\n",
        "      skip_special_tokens=True)\n",
        "\n",
        "  print(dash_line)\n",
        "  print('Example ',i+1)\n",
        "  print(dash_line)\n",
        "  print('Input Dialogue:')\n",
        "  print(dialogue)\n",
        "  print(dash_line)\n",
        "  print('Baseline human summary:')\n",
        "  print(summary)\n",
        "  print(dash_line)\n",
        "  print('Model generation without prompt engineering:')\n",
        "  print(output)\n",
        "  print()\n"
      ],
      "metadata": {
        "id": "Z70np0euuuKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summarize Dialogue with an Instruction Prompt\n",
        "\n",
        "PE is an imp concept in using Foundation Models for text generation.\n",
        "\n",
        "1. Zero Shot inference with an instruction prompt\n",
        "\n",
        "in order to instruct the model to perform a task - summarize a dialogue - take the dialogue and convert that into an instruction prompt. This is called Zero Shot Inference.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LyQoL1nl3WX4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, index in enumerate(example_indices):\n",
        "  dialogue = dataset['test'][index]['dialogue']\n",
        "  summary = dataset['test'][index]['summary']\n",
        "\n",
        "  prompt = f\"\"\"\n",
        "  Summarize the following conversation.\n",
        "  {dialogue}\n",
        "\n",
        "  Summary:\n",
        "  \"\"\"\n",
        "\n",
        "  inputs = tokenizer(prompt, return_tensors='pt')\n",
        "  output = tokenizer.decode(\n",
        "      model.generate(inputs['input_ids'], max_new_tokens=50)[0],\n",
        "      skip_special_tokens=True)\n",
        "\n",
        "  print(dash_line)\n",
        "  print('Example ',i+1)\n",
        "  print(dash_line)\n",
        "  print('Input Dialogue:')\n",
        "  print(dialogue)\n",
        "  print(dash_line)\n",
        "  print('Baseline human summary:')\n",
        "  print(summary)\n",
        "  print(dash_line)\n",
        "  print('Model generation with Zero Shot inference prompt engineering:')\n",
        "  print(output)\n",
        "  print()"
      ],
      "metadata": {
        "id": "XGJEvbS52Bsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_prompt(example_indices_full, example_index_to_summarize):\n",
        "  prompt = ''\n",
        "  for index in example_indices_full:\n",
        "    dialogue = dataset['test'][index]['dialogue']\n",
        "    summary = dataset['test'][index]['summary']\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Dialogue:\n",
        "    {dialogue}\n",
        "\n",
        "    What was going on?\n",
        "    {summary}\n",
        "    \"\"\"\n",
        "  dialogue = dataset['test'][example_index_to_summarize]['dialogue']\n",
        "\n",
        "  prompt += f\"\"\"\n",
        "  Dialogue:\n",
        "  {dialogue}\n",
        "\n",
        "  What was going on?\n",
        "  \"\"\"\n",
        "  return prompt"
      ],
      "metadata": {
        "id": "-XK6VgWV6tdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_indices_full = [80]\n",
        "example_index_to_summarize = 200\n",
        "\n",
        "one_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
        "\n",
        "print(one_shot_prompt)"
      ],
      "metadata": {
        "id": "TeZ5-TbN9Aff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary = dataset['test'][example_index_to_summarize]['summary']\n",
        "\n",
        "inputs = tokenizer(one_shot_prompt, return_tensors='pt')\n",
        "output = tokenizer.decode(\n",
        "    model.generate(inputs['input_ids'], max_new_tokens=50)[0],\n",
        "    skip_special_tokens=True)\n",
        "\n",
        "print(dash_line)\n",
        "print('Example ',i+1)\n",
        "print(dash_line)\n",
        "print('Input Dialogue:')\n",
        "print(dialogue)\n",
        "print(dash_line)\n",
        "print('Baseline human summary:\\n{summary}\\n')\n",
        "print(dash_line)\n",
        "print('Model generation with a One Shot inference prompt engineering:\\n{output}\\n')\n",
        "\n",
        "print()"
      ],
      "metadata": {
        "id": "Nr2-p_e__krG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-1_EEEGkRbia"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}