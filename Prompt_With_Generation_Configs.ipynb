{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMpwJs3qtt10DfxiyMr0pX4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/padmathanumoorthy/gen-ai-prompt-engineering/blob/master/Prompt_With_Generation_Configs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install PIP"
      ],
      "metadata": {
        "id": "M-wjsxptocRS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5Wr0JZtcsEV",
        "outputId": "59d86d81-167b-4041-9a02-c1647581c075"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-24.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 23.1.2\n",
            "    Uninstalling pip-23.1.2:\n",
            "      Successfully uninstalled pip-23.1.2\n",
            "Successfully installed pip-24.0\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --disable-pip-version-check torch==1.13.1 torchdata==0.5.1 --quiet\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gt7llFMXc6jz",
        "outputId": "b4611e89-6287-4b1e-b91e-10f4509504e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchtext 0.16.0 requires torchdata==0.7.0, but you have torchdata 0.5.1 which is incompatible.\n",
            "torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install huggingface Transformer library"
      ],
      "metadata": {
        "id": "1vgrN8OAni_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers==4.27.2 datasets==2.11.0 --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzoblHnTdiG8",
        "outputId": "55622c67-0a69-4001-bd7f-0f1fd0198be2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.7/468.7 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the datasets, LLM, tokenizer, configurator"
      ],
      "metadata": {
        "id": "i_6bD8-ini_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import GenerationConfig"
      ],
      "metadata": {
        "id": "U73FQP91d1o3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading DataSet\n",
        "\n",
        "Generating the summary of a dialogue with the pre-trained LLM Flan T5 from Huggingface. The list of available models in the HuggingFace *transformers* package can be found here:\n",
        "\n",
        "Upload simple dialogues from the Dialogsum huggingface dataset.\n",
        "This contains 10000+dialogue and the corresponding manually labeled summaries and topics."
      ],
      "metadata": {
        "id": "7tb6SBmWlLNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
        "#public dataset from huggingface\n",
        "dataset = load_dataset(huggingface_dataset_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400,
          "referenced_widgets": [
            "00bd831bf38948c9bbd10bafa6fdc6c8",
            "f3fba9c41e1542beb94ce6cad7ef6231",
            "d35d76d44ccd4ee384959b2ad848a0e0",
            "85f3f4a3e326482caf496b058393b65a",
            "f9fa14abeffc4752b03fce321ac93721",
            "05371328b0e642ef9816db98481cb415",
            "c6b3121a4a5a40a198aa7a176268eca0",
            "b0a9ff9165da42c884fd675c628388fc",
            "956cf54c0d274160a2ee096af2f5d54a",
            "693a7eb8bc69463eafdb8553c60e5e23",
            "59af8aa3d6d645ee94de4ae9513ba396",
            "cfb102ab60bd49149abfcc270af75474",
            "0de181bb01bc4f7d86310de7a67ba7cd",
            "5d12fe86c91c45d3acf40deaf35e8870",
            "51e5c7bd45214c709a9c8c8e8e32d41d",
            "962def3d1464450d98a0a65bce036ae8",
            "f223d855b3d14a15a514c91f7e3736c8",
            "1d02e9e1ff854d4f9de0829762fba891",
            "4787b5149f1d478d8bff84cecd80d485",
            "90942fecec2c41f3bdd38ddbac60b462",
            "bb6dc017f122487e88f6f4e48fa861dd",
            "f8e4b725854d4f7a9e7276e6a8ff91da",
            "a54e0c40862f475ab7efe7f78fd8e7f6",
            "3b91ab85e3b447ec8883d030326fa014",
            "053b48d49f2c49f280f62206aaec1b5c",
            "34b058147fc544e3b202a4bee00544b3",
            "4c6082aea77741729cdf0c971dd4c6bb",
            "4af6e352eef049aea9c11aef2ce8b625",
            "58e32a6de53440069bc927b949f2c4cb",
            "825d4f1a080e4567b9b9ae71e93b721d",
            "1b05fc15f3744e8eb9c2a0a6755ac753",
            "c1929653193c4bae99095af33d10dbcf",
            "7a8d6decbba64eba811aef8138c8ff0f",
            "b944eb1b64dd47a7b31cb8d2fe3f7ee6",
            "a13a7b71005641a8ae0d40da9e7df42b",
            "ec0be5640d21468d83f173427a2a5841",
            "db30a7dbc5c14011a261c84d15822c86",
            "64d50640e829468eb6107f4d47791dd2",
            "ba8bfc60f9d54d92974a437e19a56c16",
            "f189bc8d94504e07bed6b58176f24be3",
            "72dc7d095bd64b71adffaa730f8f670f",
            "9d849900d3ba42d990eeef6b2f3e4212",
            "eb8c7eee4446400b84dc304f72694f6e",
            "53161300369744ab90127289d14a525d",
            "e29c91f541974ed88c9c0608da73e355",
            "017b9075420f41f89a493a1e63dd8013",
            "e4ca9de1942a4c4bb3c9c8ed5fb70059",
            "193134fcb5244b188146ce9290e1a079",
            "87d57ad280d84ffb96608b82d029f582",
            "3502264661e4471283e1d09032246660",
            "7edbfb41d721416fa82b2c043a0c4ddb",
            "32798cd0399448dda5b6f32a3251a106",
            "aec8c23f15514fc1b56d990fd98b3dbb",
            "65a00aa7991d43a88321d8a0af012d9e",
            "cf33b904578246c4b243dfdac4e85eb0",
            "3ae782d26cf648718bd7e02cc483911e",
            "014db6a6ea844dc299880805329e4c27",
            "f67f0a22a4bb4956849c79e729dc0e51",
            "a507b8f25c684308a5f1b8bd1e56a0aa",
            "0d051b7a62b34ffb9a196e1ce60c7743",
            "342f460d8e7d4aaab8af46223d00193e",
            "d667e0cba33f4bf29eedec3162246e4a",
            "5ca277f3cc8a4950bd0861f9af3f2340",
            "038392d65cb34e42878e7adf9d435b6f",
            "2112fbcacab7471680ffdc28028b9039",
            "0f8edcd5a9f64f4fbe3885d868614544",
            "4d9bddfe5cbd45f1a06ed167dbc3fbc3",
            "8666c48845b24c3ba47c86f1933e792a",
            "029252c948684702a5c108aa32ce5201",
            "961731c2391642ac8bb0b0591475ce50",
            "3e9ee5cecca84645ab6bffc02cb48bf6",
            "c102a0f2e1064dab864602dbd24ef5ae",
            "67939ea193d94bbea04c89e4b1a559d0",
            "4f38f2f5d8a24ecba987155db0b6846c",
            "2605ef667bf4463d8e7e8d769b407a7b",
            "867700479a814ef9b41d52a4f77a14ee",
            "c1a094c725c54a0b8da8aae0df5afa57",
            "b9a5d142b05e4d8c9d45850f271a0af4",
            "6a784b2fb36046eaa3e636ca88d1f0ec",
            "c2975cb1f5df4173a2df7530484a08fb",
            "c85808d4cf694326bc71d60b5151c02b",
            "b10fca280cb94560ba704585d062f2b0",
            "7b186e436a24436db9c5c860b2fd68b6",
            "c51ea22ea96e4d5b83b244d5eef188a2",
            "4d72f6fe18ef4e25b7eb20ec910f0dd6",
            "97df5a4f89ff4a56ae43254009fcb837",
            "f2fdce84d4cb4e9ead6279c443e31530",
            "e041a72681a2498580f497143178dc8a",
            "f5cab1c2c7734e27b0fcfc9071982b7f",
            "1a472a65fd8b44b3a2b840ad178db5bb",
            "4a365369bf204a798af3c859d0967276",
            "330c585c09fb4a958e5220f49a2bfa4b",
            "d30505beaed9470799adf088097fef03",
            "94f368e2ecb14b3896804b859f576a7c",
            "2dc5fa09f4b846f0a2c604ec9d96c5d2",
            "387a8996045643d78f0018912a998a4a",
            "a2e091919f6c4349ba624d5e80043853",
            "8e944f70f5ef41349dc7142fd6656ef8",
            "86a3b4ba6fee42ccb79e2ae91078a19a",
            "43cdf56e6dd64ee99ce85eb7104def2b",
            "31d86b188f724022b4d4957ac0005ac4",
            "6c0e9fd49cb64756a08cc395e28ffd00",
            "4da05d85cd1b4c3fb73ce7ea20c1b9cf",
            "b616626c66c64e138ab6764188809eb6",
            "a07b6ec4245f4a2a93eea5f28126c229",
            "000eaf77bf97475bbebb4c16b73a49fa",
            "35e19ec1e643446aac160d6f90de41ca",
            "135fc5e58a3c47f4b0147bcf5809a860",
            "f9820a60a23d4590902eeebd42807734",
            "d2fe11a1fa2e44e58e40e51ec4ea3bc8"
          ]
        },
        "id": "6tKoJh1oeLpk",
        "outputId": "a024d8c5-c958-4b19-ca02-f3d743985a5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/4.65k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "00bd831bf38948c9bbd10bafa6fdc6c8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing dataset csv/knkarthick--dialogsum to /root/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-cd36827d3490488d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cfb102ab60bd49149abfcc270af75474"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/11.3M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a54e0c40862f475ab7efe7f78fd8e7f6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/1.35M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b944eb1b64dd47a7b31cb8d2fe3f7ee6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/442k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e29c91f541974ed88c9c0608da73e355"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3ae782d26cf648718bd7e02cc483911e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4d9bddfe5cbd45f1a06ed167dbc3fbc3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b9a5d142b05e4d8c9d45850f271a0af4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f5cab1c2c7734e27b0fcfc9071982b7f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-cd36827d3490488d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "43cdf56e6dd64ee99ce85eb7104def2b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tvKLys-FqRtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the FLAN T5 model"
      ],
      "metadata": {
        "id": "VcDV2orcpukl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load the Flan t5 model creating an instance of the AutoModelForSeq2SeqLM class with the from_pretrained method\n",
        "model_name = 'google/flan-t5-base'\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "49a78325499d4176b50092b6c73e826a",
            "942528268df44d728b0173b2438e0c68",
            "1141daf2784e466298cb349a39f62fac",
            "843b59e6fe444c1ab5fda13358dc7b8d",
            "afa990d4cae54af792e5256314d7eadd",
            "3391049a729c45838a4db8711bace1af",
            "a654dfe045674973a12d756530b40e48",
            "1412825e0d1a4e0b8cbf22f9fe45b3ce",
            "9eec517aac5e40b8970f7d89e1a3bfca",
            "3d869ab2f7fd409b83d3c1466dbf440f",
            "2091920abc854c9eb014e74bc22e0ca8",
            "8f0894da66ba44f4a488875a47f016c0",
            "fedddf8dd1604055958446641d5c6f47",
            "02fd63dad3d74a0eb0908a6a7e47895a",
            "af5067ba0f934326802eb833536ecda6",
            "22e2fa2913024e16aaf848c52516f293",
            "1ac578829569455db46f1b08404b3b7a",
            "102b6de76eed4d8aa259b42d6bf7614a",
            "c98734def7b64d5a8d284b08e2221379",
            "409f08c699d44780a2d4fbbf8a427772",
            "281d2a09c31d4a1ca51c85d7baa243ef",
            "3904620bb95b4b9f8e6b6355121da214",
            "b9a00acf58dd4f08b8f6cd7f0114a54f",
            "3085cd66151040a5a94d62bd4dcf891b",
            "6eab09d75eca4c0281ee6d978a8fb927",
            "35fdfbf20bd54a539c17fd2e2c7ad34a",
            "ed2f528db6cf4f6ab32e64f4fcab10ea",
            "d63af0d1d16f4eca91d329f988d8a6e3",
            "28ac4d37b7f34233a7a5f90caddfda68",
            "8e3ee1eb4ac54914ac51c32972ad2a2c",
            "2fcc210297fd4e08be6d94814e00bd7a",
            "09514a6c22f24c94ac427b995a77c390",
            "cf60aa3ebbf74e67a87dd28c12b5f7b7"
          ]
        },
        "id": "MnwGm741f_LM",
        "outputId": "754f15b3-88e5-4a93-9978-5549f9e1b94a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "49a78325499d4176b50092b6c73e826a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8f0894da66ba44f4a488875a47f016c0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b9a00acf58dd4f08b8f6cd7f0114a54f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instantiate the Tokenizer for encode and decode"
      ],
      "metadata": {
        "id": "LxLgE3Xnp1fl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#To perform encoding and decoding, we need to work with text in a tokenized form.\n",
        "#Tokenization is the process of splitting texts into smaller units that can be\n",
        "#processed by the LLM models\n",
        "\n",
        "#Download the tokenizer for the Flan-t5 model using transformer package AutoTokenizer.from_pretrained() method,\n",
        "#Parameter use_fast switches on fast Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "6211c88d34e44b07a428c0e057448292",
            "7b43fb2d69b743efbcf595c289663163",
            "b17266549a624f54b7bd436a17a9ce7c",
            "dac2d4484ec5470d925d9266099ae5b5",
            "1604c78edf334788a9b3c42f8d3f1c0e",
            "d0a8fd2980524021a9814abe1d5bd159",
            "549ac40544704639bb6420d413637135",
            "d79f8ce6ac184c72aa221552ea1a33a0",
            "414216264c9f49dca3044977cf792cb3",
            "2cce77baa311429bb5293ebff3eccd29",
            "6b54165be4c7478f9eae628c22e92b0c",
            "8e2b57dabe2c402cbc63251ffeb48d14",
            "9f37547fe68f463b82ab319c6e39b5bd",
            "9a6920917c8a436c9a7c907175d8f374",
            "1a98c97c16554f7aa38c139206a19395",
            "df7c3b2e95b4486aa9f1c7a8e078aba5",
            "8613b3452c134a80b44eccc06af9547c",
            "e9dda8cf2094430a8828ca84146c628d",
            "e6be2d6599c94a1b870e8b7b34f65a78",
            "100847dccba24653974676b0bbd345f7",
            "1eea0b68f12a4c8a9d918cd0c031cd8d",
            "b5790d1ffc9743bea54b339e910cde37",
            "c3b669f4774647dcaa483049bb3b9ba0",
            "865c40318b594a0f847bb4a9b3019648",
            "d7ee3f0ea1a649f48c5b10a1c4ea863c",
            "49d33a980b1742c9ae7d00492ec61b18",
            "e1bfd3617e904d5298fc9598dd7ff561",
            "f84b0e2c86194e759c4404d246866b5f",
            "4818c7630c5b4051a10e47434216835b",
            "dc013fc0c622494b823858c5a7c74596",
            "9dec9cb5e0ef4444ac6a10579a625789",
            "ea7388e5b6d9462c97d0d86a97993f0a",
            "2faf7b284c1a49d59fd34e7b37324e08",
            "d56a11c846114b9190189485f8be061c",
            "7ac774c905f641f9b9bb2c4e9c6b58cd",
            "5217085b11544757a8d1e7f84944f378",
            "203790e5853b43eab54e9b169d7b949a",
            "dc5669904d654b5688f3a589fa40891c",
            "7449edc35a464df3887a889dd6df9a3f",
            "0fc782310d894ac5b9e39dff0b8c9256",
            "396479e961e543a9bca33b0d192a910c",
            "039e78d60e5440cca16031b6628ac54d",
            "347d72f83da944a3a38676aa3e8cd412",
            "e7f91f381cff4c35ac13b5c14630eb37"
          ]
        },
        "id": "lF_ZSKcBjF5d",
        "outputId": "9aa1ca02-072f-4544-f324-a6d9b8be7751"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6211c88d34e44b07a428c0e057448292"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8e2b57dabe2c402cbc63251ffeb48d14"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c3b669f4774647dcaa483049bb3b9ba0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d56a11c846114b9190189485f8be061c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#dash_line = '-'.join('' for x in range(100))\n",
        "dash_line = '-'*100"
      ],
      "metadata": {
        "id": "xSq4oImNsUxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Two or Few Shot Inference"
      ],
      "metadata": {
        "id": "3CPLxUe0bxQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_prompt(example_indices_full, example_index_to_summarize):\n",
        "  prompt = ''\n",
        "  for index in example_indices_full:\n",
        "    dialogue = dataset['test'][index]['dialogue']\n",
        "    summary = dataset['test'][index]['summary']\n",
        "\n",
        "    prompt += f\"\"\"\n",
        "    Dialogue:\n",
        "    {dialogue}\n",
        "\n",
        "    What was going on?\n",
        "    {summary}\n",
        "    \"\"\"\n",
        "  dialogue = dataset['test'][example_index_to_summarize]['dialogue']\n",
        "\n",
        "  prompt += f\"\"\"\n",
        "  Dialogue:\n",
        "  {dialogue}\n",
        "\n",
        "  What was going on?\n",
        "  \"\"\"\n",
        "  return prompt"
      ],
      "metadata": {
        "id": "-XK6VgWV6tdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_indices_full = [100,200]\n",
        "\n",
        "example_index_to_summarize = 150\n",
        "\n",
        "one_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
        "\n",
        "print(one_shot_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeZ5-TbN9Aff",
        "outputId": "8fe983a2-dfe9-4d36-b3a0-83f1c89d86ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Dialogue:\n",
            "    #Person1#: OK, that's a cut! Let's start from the beginning, everyone.\n",
            "#Person2#: What was the problem that time?\n",
            "#Person1#: The feeling was all wrong, Mike. She is telling you that she doesn't want to see you any more, but I want to get more anger from you. You're acting hurt and sad, but that's not how your character would act in this situation.\n",
            "#Person2#: But Jason and Laura have been together for three years. Don't you think his reaction would be one of both anger and sadness?\n",
            "#Person1#: At this point, no. I think he would react the way most guys would, and then later on, we would see his real feelings.\n",
            "#Person2#: I'm not so sure about that.\n",
            "#Person1#: Let's try it my way, and you can see how you feel when you're saying your lines. After that, if it still doesn't feel right, we can try something else.\n",
            "\n",
            "    What was going on?\n",
            "    #Person1# and Mike have a disagreement on how to act out a scene. #Person1# proposes that Mike can try to act in #Person1#'s way.\n",
            "    \n",
            "    Dialogue:\n",
            "    #Person1#: Have you considered upgrading your system?\n",
            "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
            "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
            "#Person2#: That would be a definite bonus.\n",
            "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
            "#Person2#: How can we do that?\n",
            "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
            "#Person2#: No.\n",
            "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
            "#Person2#: That sounds great. Thanks.\n",
            "\n",
            "    What was going on?\n",
            "    #Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
            "    \n",
            "  Dialogue:\n",
            "  #Person1#: Taxi!\n",
            "#Person2#: Where will you go, sir?\n",
            "#Person1#: Friendship Hotel.\n",
            "#Person2#: OK, it's not far from here.\n",
            "#Person1#: I have something important to do, can you fast the speed?\n",
            "#Person2#: Sure, I'll try my best. Here we are.\n",
            "#Person1#: It's fast! How much should I pay you?\n",
            "#Person2#: The reading on the meter is 15 yuan.\n",
            "#Person1#: Here's 20 yuan, keep the change.\n",
            "#Person2#: Thank you very much.\n",
            "\n",
            "  What was going on?\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dialogue = dataset['test'][example_index_to_summarize]['dialogue']\n",
        "summary = dataset['test'][example_index_to_summarize]['summary']\n",
        "\n",
        "inputs = tokenizer(one_shot_prompt, return_tensors='pt')\n",
        "output = tokenizer.decode(\n",
        "    model.generate(inputs['input_ids'], max_new_tokens=50)[0],\n",
        "    skip_special_tokens=True)\n",
        "\n",
        "print(dash_line)\n",
        "print('Example ')\n",
        "print(dash_line)\n",
        "print('Input Dialogue:')\n",
        "print(dialogue)\n",
        "print(dash_line)\n",
        "print(\"Baseline human summary:\")\n",
        "print(summary)\n",
        "print(dash_line)\n",
        "print('Model generation with a Two/Few Shot inference prompt engineering:')\n",
        "print(output)\n",
        "print(dash_line)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nr2-p_e__krG",
        "outputId": "5fa3f926-5fe7-4d5d-c2f9-fd5d351301d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (703 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "Example \n",
            "----------------------------------------------------------------------------------------------------\n",
            "Input Dialogue:\n",
            "#Person1#: Taxi!\n",
            "#Person2#: Where will you go, sir?\n",
            "#Person1#: Friendship Hotel.\n",
            "#Person2#: OK, it's not far from here.\n",
            "#Person1#: I have something important to do, can you fast the speed?\n",
            "#Person2#: Sure, I'll try my best. Here we are.\n",
            "#Person1#: It's fast! How much should I pay you?\n",
            "#Person2#: The reading on the meter is 15 yuan.\n",
            "#Person1#: Here's 20 yuan, keep the change.\n",
            "#Person2#: Thank you very much.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Baseline human summary:\n",
            "#Person1# takes a taxi to the Friendship Hotel for something important.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Model generation with a Two/Few Shot inference prompt engineering:\n",
            "The taxi driver will take Person1 to Friendship Hotel at a speed of 15 yuan.\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Engineering with Generation Configs\n",
        "\n",
        "If temperature is high, output will be more creative"
      ],
      "metadata": {
        "id": "Ab-mzfCfEBJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generation_config = GenerationConfig (max_new_tokens=50)\n",
        "#generation_config = GenerationConfig (max_new_tokens=50, do_sample=True, temperature=1.0)\n",
        "#generation_config = GenerationConfig (max_new_tokens=50, do_sample=True, temperature=0.1)\n",
        "#generation_config = GenerationConfig (max_new_tokens=50, do_sample=True, temperature=0.5)\n",
        "\n",
        "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
        "output = tokenizer.decode(\n",
        "    model.generate(inputs['input_ids'],\n",
        "                   #max_new_tokens=50,\n",
        "                   generation_config=generation_config)[0],\n",
        "                  skip_special_tokens=True)\n",
        "\n",
        "print(dash_line)\n",
        "print('Example ',i+1)\n",
        "print(dash_line)\n",
        "print('Input Dialogue:')\n",
        "print(dialogue)\n",
        "print(dash_line)\n",
        "print(\"Baseline human summary:\")\n",
        "print(summary)\n",
        "print(dash_line)\n",
        "print('Model generation with a Two/Few Shot inference prompt engineering:')\n",
        "print(output)\n",
        "print(dash_line)"
      ],
      "metadata": {
        "id": "SaCM1lm0EAcm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}